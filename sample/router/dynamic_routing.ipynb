{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whcui/.pyenv/versions/miniconda3-3.8-4.11.0/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_x: tensor([[-0.3391, -0.7984, -0.1172, -0.6895, -0.3774, -0.0508,  0.7085,  0.6831,\n",
      "          0.3684,  0.0477],\n",
      "        [ 0.1209,  0.1663, -0.7751, -1.3780,  0.0835,  0.7720,  1.0615,  2.0556,\n",
      "         -0.5559,  0.0347],\n",
      "        [ 0.6359, -1.6873,  0.4640, -0.5502, -0.6227, -0.9929,  0.8678,  1.6215,\n",
      "          1.6093,  1.6121],\n",
      "        [-0.4614,  0.1946,  0.5785,  0.4891, -0.8515,  0.6766, -0.2434, -0.5327,\n",
      "          0.4611, -0.8400]], device='cuda:0')\n",
      "GridTensor([[-0.6194, -0.8369,  0.1094, -0.1937,  0.4171,  0.0740,  0.2162,\n",
      "              0.8904, -0.8934, -0.8794]], device='cuda:0')\n",
      "tag_stack: [tensor([4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W TensorAdvancedIndexing.cpp:1550] Warning: scatter_reduce() is in beta and the API may change at any time. (function operator())\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.annotator = Annotator([0])\n",
    "        self.scatter_router = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)  # keep\n",
    "        self.expert2 = nn.Linear(10, 10)  # upsample\n",
    "        self.gather_router = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.annotator(x)\n",
    "        x_gates = self.route_func(x)\n",
    "        route_results_x = self.scatter_router(x, x_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        x = self.gather_router([x_0, x_1])\n",
    "        return x\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(1):\n",
    "    in_x = torch.randn((4, 10)).cuda()\n",
    "    print(f\"in_x: {in_x}\")\n",
    "    dy_model = dy_model.cuda().eval()\n",
    "    with torch.inference_mode():\n",
    "        results = dy_model(in_x.cuda())\n",
    "    print(results)\n",
    "\n",
    "from brt import symbolic_trace\n",
    "\n",
    "traced_dy_model = symbolic_trace(dy_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Default Dispatcher and 2-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======init inputs=======\n",
      "tensor([[-1.1749, -0.4041,  0.8114, -0.4443,  0.0035,  0.9211,  1.3677, -1.4528,\n",
      "          0.8938,  1.5142],\n",
      "        [ 0.0856,  1.5622, -0.6412,  1.0929, -0.4518, -1.1815,  0.0976,  0.3586,\n",
      "         -1.1757,  0.4791],\n",
      "        [-0.5621, -0.8564,  1.2285, -0.1026, -2.1095,  0.8689,  0.2859, -0.4830,\n",
      "          1.8036,  1.0846],\n",
      "        [-0.8266, -0.8269,  0.7378, -2.4536,  1.6403,  0.5254, -1.0185,  1.1363,\n",
      "         -0.9528,  0.5939]], device='cuda:0')\n",
      "tensor([[ 0.5976, -0.6808,  1.2866,  0.5877, -0.7214,  0.2938,  0.8010, -0.3186,\n",
      "         -0.0925,  0.6383],\n",
      "        [-0.5206, -1.3267,  0.6838, -1.4214, -0.7063, -1.7512, -0.3362,  0.2202,\n",
      "          0.4085,  0.5689],\n",
      "        [-0.3622, -1.0420, -1.5170,  1.5639, -0.2891, -0.5698, -1.0670,  0.3363,\n",
      "          1.1127, -0.0434],\n",
      "        [ 0.5764, -0.1310,  0.3552,  0.9449,  0.2095, -0.3974,  0.3978, -0.5305,\n",
      "          2.3572,  0.5395]], device='cuda:0')\n",
      "=======scatter results=======\n",
      "routed results x_0: GridTensor([[-0.5621, -0.8564,  1.2285, -0.1026, -2.1095,  0.8689,  0.2859,\n",
      "             -0.4830,  1.8036,  1.0846]], device='cuda:0')\n",
      "tag_stack: [tensor([3], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "routed results x_1: GridTensor([[-1.1749, -0.4041,  0.8114, -0.4443,  0.0035,  0.9211,  1.3677,\n",
      "             -1.4528,  0.8938,  1.5142]], device='cuda:0')\n",
      "tag_stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "routed results y_0: GridTensor([[ 0.5976, -0.6808,  1.2866,  0.5877, -0.7214,  0.2938,  0.8010,\n",
      "             -0.3186, -0.0925,  0.6383],\n",
      "            [ 0.5764, -0.1310,  0.3552,  0.9449,  0.2095, -0.3974,  0.3978,\n",
      "             -0.5305,  2.3572,  0.5395]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "routed results y_1: GridTensor([[ 0.5976, -0.6808,  1.2866,  0.5877, -0.7214,  0.2938,  0.8010,\n",
      "             -0.3186, -0.0925,  0.6383],\n",
      "            [ 0.5764, -0.1310,  0.3552,  0.9449,  0.2095, -0.3974,  0.3978,\n",
      "             -0.5305,  2.3572,  0.5395]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "=======final results=======\n",
      "(GridTensor([[-0.1233,  0.3525, -0.7045,  0.0117,  0.0362, -0.5238,  0.8791,\n",
      "              0.4085, -0.3057, -0.0958],\n",
      "            [ 0.3631, -0.7767, -1.2200, -0.1729, -0.5242, -0.5576, -0.4361,\n",
      "             -0.9416,  0.0566, -0.2852],\n",
      "            [ 0.2117,  0.8079,  0.0978,  0.2484,  0.2889, -0.7682,  1.1165,\n",
      "              0.3412, -0.2147,  0.0592]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 3, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([3], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}, GridTensor([[ 0.1072, -0.3361, -0.2426, -0.7844, -0.6884,  0.1482, -0.9716,\n",
      "             -0.5492,  0.6463,  0.7515,  0.1994,  0.8768,  0.1431,  0.4833,\n",
      "              0.2871, -0.7856, -0.6880,  0.6086,  0.7363,  0.6981],\n",
      "            [ 0.3995, -0.7083, -0.3315,  0.2195, -0.2185,  0.4799,  0.4311,\n",
      "             -0.3858,  0.4783, -0.1320,  0.5199, -0.0049,  0.6884, -0.8141,\n",
      "              0.4254, -0.9119, -0.5593,  0.5641,  0.2220, -0.9806]],\n",
      "           device='cuda:0')\n",
      "tag_stack: [tensor([1, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.annotator = Annotator([0])\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)  # keep\n",
    "        self.expert2 = nn.Linear(10, 20)  # upsample\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.annotator(x)\n",
    "        y = self.annotator(y)\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        print(\"=======scatter results=======\")\n",
    "        print(f\"routed results x_0: {route_results_x[0]}\")\n",
    "        print(f\"routed results x_1: {route_results_x[1]}\")\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        print(f\"routed results y_0: {route_results_y[0]}\")\n",
    "        print(f\"routed results y_1: {route_results_y[1]}\")\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(1):\n",
    "    in_x = torch.randn((4, 10)).cuda()\n",
    "    in_y = torch.randn((4, 10)).cuda()\n",
    "    print(\"=======init inputs=======\")\n",
    "    print(in_x)\n",
    "    print(in_y)\n",
    "\n",
    "    dy_model = dy_model.cuda().eval()\n",
    "    with torch.inference_mode():\n",
    "        results = dy_model(in_x.cuda(), in_y.cuda())\n",
    "    print(\"=======final results=======\")\n",
    "    print(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======gather inputs=======\n",
      "x_0: GridTensor([[ 0.1091, -0.9170,  0.4356, -0.3602, -0.4261, -0.7068, -0.3975,\n",
      "             -0.4991,  0.8470,  0.9959],\n",
      "            [-0.3384, -0.9381, -0.2078,  0.1462,  0.0573, -0.2604, -0.5532,\n",
      "             -0.3802,  0.9303,  0.5089]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 3], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "y_0: GridTensor([[-1.3767, -0.2169,  0.2297,  0.3003, -0.0279, -0.2192,  0.5845,\n",
      "             -0.6014, -0.2571, -0.6309],\n",
      "            [-1.0285,  0.4932, -0.6910,  1.0688, -0.4522,  0.1841, -0.1227,\n",
      "             -1.5571, -0.1911, -0.1539],\n",
      "            [-0.2268,  0.4516,  0.9427, -0.1094,  0.0927, -1.0385,  0.5325,\n",
      "              0.1608, -0.1912, -0.4807]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 2, 3], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([3], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "x_1: GridTensor([[-0.3686, -0.2849,  0.4628, -0.3495, -0.5090,  0.2628,  0.1521,\n",
      "             -0.4330,  0.1048, -0.7074, -0.0274,  0.6480,  0.3073,  1.3587,\n",
      "             -0.3405,  0.4778,  0.2561, -0.2017,  0.4428,  0.2824]],\n",
      "           device='cuda:0')\n",
      "tag_stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "y_1: GridTensor([], device='cuda:0', size=(0, 20))\n",
      "tag_stack: [tensor([], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([0], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "GridTensor([[-1.2676, -1.1338,  0.6653, -0.0600, -0.4540, -0.9260,  0.1870,\n",
      "             -1.1005,  0.5899,  0.3650],\n",
      "            [-1.0285,  0.4932, -0.6910,  1.0688, -0.4522,  0.1841, -0.1227,\n",
      "             -1.5571, -0.1911, -0.1539],\n",
      "            [-0.5652, -0.4865,  0.7349,  0.0368,  0.1499, -1.2989, -0.0207,\n",
      "             -0.2193,  0.7391,  0.0282]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 2, 3], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([3], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "GridTensor([[-0.3686, -0.2849,  0.4628, -0.3495, -0.5090,  0.2628,  0.1521,\n",
      "             -0.4330,  0.1048, -0.7074, -0.0274,  0.6480,  0.3073,  1.3587,\n",
      "             -0.3405,  0.4778,  0.2561, -0.2017,  0.4428,  0.2824]],\n",
      "           device='cuda:0')\n",
      "tag_stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.annotator = Annotator([0])\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "            protocol_kwargs={\"residual_path\": 0},\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "            protocol_kwargs={\"residual_path\": 0},\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.annotator(x)\n",
    "        y = self.annotator(y)\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        print(\"=======gather inputs=======\")\n",
    "        print(f\"x_0: {x_0}\")\n",
    "        print(f\"y_0: {y_0}\")\n",
    "        print(f\"x_1: {x_1}\")\n",
    "        print(f\"y_1: {y_1}\")\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model = dy_model.cuda().eval()\n",
    "\n",
    "for i in range(1):\n",
    "    x = torch.randn((3, 10)).cuda()\n",
    "    y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        x, y = dy_model(x, y)\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor while routing gates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridTensor([[ 0.9916,  0.3189, -0.1299, -1.2428, -1.0203,  0.4072, -0.3563,\n",
      "             -1.3071, -0.7796,  0.2188]], device='cuda:0')\n",
      "tag_stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([1], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "GridTensor([[-0.5785,  0.1877,  0.5272,  0.6567,  0.7705,  0.0801, -0.3151,\n",
      "             -0.9948, -0.0836,  1.4111, -0.3184,  0.1045, -0.1758, -0.3024,\n",
      "              0.7401,  0.7916,  0.1828, -1.0672,  0.7433,  0.3995],\n",
      "            [ 0.7639,  0.8094, -1.4858, -0.9996, -0.9652,  0.3988,  0.5664,\n",
      "             -0.5325, -0.1345, -1.1464,  0.2782, -0.1364,  0.2080,  0.9035,\n",
      "              0.7719, -0.6144, -1.2163,  1.3061, -0.0617, -0.3396],\n",
      "            [ 0.3861,  0.2587, -0.1797, -0.1789,  0.3552,  0.1392, -0.0086,\n",
      "             -0.2502,  0.0438, -0.3467, -0.3436,  0.0415, -0.2150,  0.4548,\n",
      "             -0.2964, -0.1291, -0.2322, -0.0439, -0.3934, -0.3841]],\n",
      "           device='cuda:0')\n",
      "tag_stack: [tensor([1, 2, 3], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([3], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.annotator = Annotator([0])\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True,\n",
    "            protocol_type=\"residual_threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "            protocol_kwargs={\"residual_path\": 0, \"threshold\": 0.5},\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.annotator(x)\n",
    "        y = self.annotator(y)\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x, route_gates_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_0 = route_gates_x[0] * x_0\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        x_1 = route_gates_x[1] * x_1\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model = dy_model.cuda().eval()\n",
    "with torch.inference_mode():\n",
    "    for i in range(1):\n",
    "        x = torch.randn((3, 10)).cuda()\n",
    "        y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "        x, y = dy_model(x, y)\n",
    "\n",
    "        print(x)\n",
    "        print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 4-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self, dst_num):\n",
    "        super().__init__()\n",
    "        self.annotator = Annotator([0])\n",
    "        self.route_func = nn.Sequential(\n",
    "            nn.Conv2d(4, 2, 1), nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(2, 2, 1),\n",
    "        )\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True, protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert2 = nn.Conv2d(4, 8, 1)\n",
    "        self.expert3 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert4 = nn.Conv2d(4, 8, 1)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.annotator(x)\n",
    "        y = self.annotator(y)\n",
    "        gates_x = self.route_func(x).view(-1, 2)\n",
    "        gates_y = self.route_func(y).view(-1, 2)\n",
    "        route_results_x, _ = self.scatter_router_0(x, gates_x)\n",
    "        route_results_y = self.scatter_router_1(y, gates_y)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting(2)\n",
    "dy_model = dy_model.cuda().eval()\n",
    "with torch.inference_mode():\n",
    "    for i in range(10):\n",
    "        x = torch.randn((3, 4, 2, 2)).cuda()\n",
    "        y = torch.randn((3, 4, 2, 2)).cuda()\n",
    "        x, y = dy_model(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import Annotator\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.annotator = Annotator([0])\n",
    "        self.scatter_router = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 10)\n",
    "        self.gather_router = GatherRouter(fabric_type=\"combine\")\n",
    "        self.route_func_1 = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "            protocol_kwargs={\"residual_path\": 0},\n",
    "        )\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.annotator(x)\n",
    "        x_gates = self.route_func(x)\n",
    "        route_results_x = self.scatter_router(x, x_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        x_gather = self.gather_router([x_0, x_1])\n",
    "        # print(f\"==============x==============\\n{x}\")\n",
    "        x_gates_1 = self.route_func_1(x_gather)\n",
    "        route_results_x_1 = self.scatter_router_1(x_gather, x_gates_1)\n",
    "        x_0 = self.expert3(route_results_x_1[0])\n",
    "        # print(f\"==============x_0==============\\n{x_0}\")\n",
    "        # print(f\"==============route_results_x_1==============\\n{route_results_x_1[1]}\")\n",
    "        x_gather_1 = self.gather_router_1([x_0, route_results_x_1[1]])\n",
    "\n",
    "        return x_gather, x_gather_1\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(10):\n",
    "    in_x = torch.randn((4, 10)).cuda()\n",
    "    dy_model = dy_model.cuda().eval()\n",
    "    with torch.inference_mode():\n",
    "        results = dy_model(in_x.cuda())\n",
    "    assert torch.allclose(results[0].tag, results[1].tag)\n",
    "    # print(f\"==============results==============\\n{results}\")\n",
    "\n",
    "from brt import symbolic_trace\n",
    "\n",
    "traced_dy_model = symbolic_trace(dy_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4914c35dbc1a262acb2241fbfc193aaeb9362d455da2cebdd4b0a1d658dbfd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
