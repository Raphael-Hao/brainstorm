{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Default Dispatcher and 2-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)  # keep\n",
    "        self.expert2 = nn.Linear(10, 20)  # upsample\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(1):\n",
    "    in_x = torch.randn((4, 10))\n",
    "    in_y = torch.randn((4, 10))\n",
    "\n",
    "    dy_model.cpu()\n",
    "    cpu_x, cpu_y = dy_model(in_x.cpu(), in_y.cpu())\n",
    "    print(cpu_x)\n",
    "    print(cpu_y)\n",
    "\n",
    "    dy_model.cuda()\n",
    "    cuda_x, cuda_y = dy_model(in_x.cuda(), in_y.cuda())\n",
    "    print(cuda_x)\n",
    "    print(cuda_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model.cuda()\n",
    "\n",
    "for i in range(1):\n",
    "    x = torch.randn((3, 10)).cuda()\n",
    "    y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "    x, y = dy_model(x, y)\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor while routing gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.4156]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3]), tensor([[0.6150],\n",
      "        [0.1975],\n",
      "        [0.4762]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[ 0.4268,  0.6731, -0.2888,  0.0502,  0.5056, -0.8375, -0.1827,\n",
      "               0.6613, -1.1024,  0.9359]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "tensor([[ 0.1345, -0.5591, -0.2105,  0.0021,  0.4726,  0.8786,  0.0605, -0.4822,\n",
      "         -0.5286, -0.4091,  0.7765, -0.7589, -0.0979, -0.1256,  0.6138,  0.3466,\n",
      "          0.8299,  0.3985, -0.1366,  0.8685],\n",
      "        [-0.0185,  0.9469, -0.2337, -0.1099,  0.0874,  0.7555,  0.9220,  0.4358,\n",
      "          0.7234, -0.0283, -0.0552,  0.2293,  0.1887, -0.7704, -0.5916, -1.0441,\n",
      "          0.7610,  0.2705,  0.5875,  0.8707],\n",
      "        [-0.3191, -1.0070,  0.0735, -1.3915,  0.1924,  0.4870,  0.9591,  0.2915,\n",
      "         -0.0794, -1.1862,  0.1343, -0.3646,  0.4463, -0.7863,  0.1748,  0.5977,\n",
      "          0.1751, -0.0834,  0.1731,  0.2174]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[tensor([[0.1990],\n",
      "        [0.0000],\n",
      "        [0.3100]], device='cuda:0', grad_fn=<AliasBackward0>), ProtoTensor(\n",
      "data: ProtoTensor([[0.4551],\n",
      "             [0.1792]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])]\n",
      "tensor([[-0.2704, -0.9667,  0.9382, -0.7270, -0.6567, -0.2035, -0.1777,  0.4544,\n",
      "          0.7014,  0.7905],\n",
      "        [-0.3447, -0.1704,  0.4037,  0.1160, -1.4074, -0.6865,  0.0270,  1.0934,\n",
      "          0.0443,  0.8443],\n",
      "        [ 0.4160,  0.3807, -0.1806,  0.6071,  0.4826, -0.2100,  0.4361, -0.1622,\n",
      "         -0.5540, -0.0297]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[ 0.9803, -0.1517,  0.9942,  0.0814,  0.5893, -0.1848,  0.7386,\n",
      "              -0.0577,  0.0421, -0.6965, -0.2018, -0.0015, -0.3648, -0.3915,\n",
      "               0.1418, -0.2388,  0.2555, -0.6460, -0.3601, -0.8893],\n",
      "             [ 0.2081, -0.1778, -0.8380, -0.4231,  0.0807, -0.7984, -0.2040,\n",
      "               0.3531, -0.9398,  1.3555,  0.8591,  1.0536,  0.0513,  0.6617,\n",
      "               0.7752,  0.4526, -0.0839, -0.1860,  0.8557,  0.3006]],\n",
      "            device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0]], device='cuda:0')]\n",
      "load stack: [3]), ProtoTensor(\n",
      "data: ProtoTensor([[0.3395],\n",
      "             [0.6621]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-0.2114, -0.1764, -0.3279,  0.2424, -0.1683, -0.0528, -0.1059,\n",
      "              -0.0545, -0.2096, -0.2909]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-1.3961, -0.2057,  0.9550, -1.1329, -0.3526,  0.3104, -0.3291,\n",
      "              -0.7464,  0.6012, -0.9299, -0.3395, -0.0799,  1.1261, -1.1779,\n",
      "               0.2260,  0.5077,  0.4199, -0.1160, -0.6677,  0.3431],\n",
      "             [-0.4637, -0.7890,  0.7775, -1.0708, -0.1158, -0.3099,  0.6322,\n",
      "              -0.2589,  0.2370, -0.4945, -0.5022,  0.3539, -0.3060,  0.2078,\n",
      "              -0.1866,  0.2198,  0.2719, -0.3592, -0.1917,  0.2335]],\n",
      "            device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 1))\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3]), tensor([[0.9572],\n",
      "        [1.2804],\n",
      "        [0.4300]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 10), grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3])\n",
      "tensor([[ 0.6339, -0.1909,  0.9959, -0.4744,  0.4374,  0.0224,  0.0746, -0.3525,\n",
      "         -0.9227,  0.5456,  0.0820,  0.3316, -0.3898,  0.0620, -0.1990, -0.5709,\n",
      "          0.1900, -0.2946, -0.5980, -0.3041],\n",
      "        [-0.1778, -0.9652,  0.6788, -0.3740,  0.7418,  1.2418,  0.6656,  0.0616,\n",
      "         -0.6241, -0.7025, -0.4727, -0.2647, -0.5413,  0.2615, -0.9318,  0.0928,\n",
      "          1.3589,  0.0666, -0.4493,  1.0941],\n",
      "        [-0.9339,  0.0663,  0.2508, -1.1800, -0.3655,  0.4250, -0.2104, -0.5236,\n",
      "         -0.0119,  0.4927,  0.2571,  0.1457,  0.5751, -0.4658, -0.0918, -0.2980,\n",
      "          0.0964,  0.3743, -0.0987,  0.9293]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.3943]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3]), tensor([[0.2966],\n",
      "        [0.0171],\n",
      "        [0.3213]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-0.3131, -0.3653,  0.0949, -0.7668, -0.1449,  0.2834, -1.3140,\n",
      "               0.3553, -0.1980,  0.1136]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "tensor([[ 1.4785e-01, -4.4557e-01,  4.8783e-02, -6.3618e-01,  2.1428e-01,\n",
      "         -5.1357e-01,  5.8821e-01, -3.0192e-01,  4.1257e-01, -6.6552e-01,\n",
      "          5.1498e-01,  3.2664e-01, -3.4873e-01, -2.2370e-01,  1.0166e+00,\n",
      "          1.3340e-01,  4.5072e-01, -4.8721e-01,  4.0330e-01,  7.7799e-02],\n",
      "        [ 7.9640e-01,  8.0928e-01,  7.2930e-01,  7.1230e-01,  2.1024e-01,\n",
      "          4.9304e-01,  1.6917e-01, -4.1777e-01,  2.2807e-01,  3.6557e-01,\n",
      "          4.4957e-02, -5.7112e-01,  1.8227e-02, -4.5381e-01, -2.5126e-01,\n",
      "         -9.1381e-01, -1.4664e-01,  2.5779e-01, -4.2722e-01, -2.4362e-01],\n",
      "        [-3.2761e-01, -2.8783e-01,  5.2721e-04, -2.9111e-01, -5.2563e-02,\n",
      "          6.1238e-01, -4.2080e-01, -9.2743e-01, -1.7040e-01, -8.3527e-01,\n",
      "          7.7356e-01, -1.0103e+00,  8.8182e-01, -1.0262e+00,  1.2068e+00,\n",
      "          7.6215e-01,  4.5839e-01,  3.8579e-01, -5.9216e-01,  2.6981e-01]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.9401],\n",
      "             [0.1924]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3]), ProtoTensor(\n",
      "data: ProtoTensor([[0.4451]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0]], device='cuda:0')]\n",
      "load stack: [3])]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-0.4487, -0.4023,  1.4317, -0.5203, -0.6182, -0.7688, -0.4617,\n",
      "               0.2442, -0.1350,  0.8485],\n",
      "             [-0.4162,  0.0410,  0.6485,  0.2183, -0.5625, -0.8768,  0.7560,\n",
      "               0.1912, -0.1942,  0.2778]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-0.4214, -0.2497,  0.2755, -0.0248, -0.4510,  0.1384,  0.1672,\n",
      "              -0.2751, -0.0217, -0.1422, -0.6103, -0.3430,  0.2841,  0.2755,\n",
      "              -0.3212,  0.6385,  0.2836,  0.3388, -0.5494,  0.3685]],\n",
      "            device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.6276]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3]), tensor([[0.8348],\n",
      "        [0.2964],\n",
      "        [0.8932]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[ 0.3684,  0.0838,  0.3602,  0.2789,  0.1675,  0.3537,  0.4459,\n",
      "               0.1237, -0.1527,  0.0647]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[1]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "tensor([[-0.6587, -0.6727, -0.1021, -1.3183, -0.2402,  0.4442,  0.4400, -0.3429,\n",
      "         -0.2316, -0.5269,  0.2540, -0.2035,  0.2517, -0.3609,  0.2298,  0.3283,\n",
      "          0.5877,  0.3044, -0.1243,  0.8486],\n",
      "        [ 0.7892, -0.1113,  0.1038,  0.4170,  0.1378, -0.7650,  0.2228,  0.1135,\n",
      "         -0.5192,  0.7437, -0.0470,  0.5062, -0.4794,  0.8793,  0.1650,  0.2327,\n",
      "         -0.0325, -0.2964,  0.0730, -0.3535],\n",
      "        [ 0.0706, -0.9216, -0.2228, -0.4887,  0.1568,  0.8976,  0.5975, -0.3784,\n",
      "         -0.6329, -0.3612,  0.4455, -0.8929, -0.4163,  0.2938,  0.0636,  0.2598,\n",
      "          0.6024,  0.6403, -0.1996,  1.0278]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.6910]], device='cuda:0', grad_fn=<AliasBackward0>), ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 1))\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3])]\n",
      "tensor([[-1.0580, -0.9820,  0.3063, -0.9850, -0.4477, -0.2865, -0.7479,  0.2268,\n",
      "         -0.2431, -0.2052],\n",
      "        [-0.6601, -0.3873,  0.3672, -0.6662, -1.1532,  0.6039, -0.4470,  0.2291,\n",
      "         -0.6576,  0.0523],\n",
      "        [ 0.9370,  0.9758, -0.5588,  1.6959,  0.0523,  0.1722, -0.0555,  0.7970,\n",
      "         -0.3813,  0.1105]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 20), grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3])\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 1))\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3]), tensor([[0.7486],\n",
      "        [0.1597],\n",
      "        [0.0683]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([], device='cuda:0', size=(0, 10), grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([], device='cuda:0', size=(0, 1), dtype=torch.int64)]\n",
      "load stack: [3])\n",
      "tensor([[-0.4630, -0.3022,  0.0842, -0.9291,  0.0081,  0.6607,  0.0038, -0.1438,\n",
      "         -0.7126,  0.5004,  0.2435,  0.0394,  0.2834, -0.0854, -0.3122, -0.0734,\n",
      "          0.3280,  0.3806, -0.1228,  0.9017],\n",
      "        [-0.6782, -1.1301, -0.3544, -0.4281, -0.4252,  0.2716, -0.5228, -0.8885,\n",
      "         -0.5512, -0.0700,  0.6716, -1.1327,  0.5233,  0.3572,  0.9627,  1.5036,\n",
      "         -0.2677,  0.7397, -0.3679,  0.7560],\n",
      "        [-0.4785,  0.3684,  0.8270,  0.2105,  0.5556,  0.0992, -0.5787, -0.3139,\n",
      "          0.5181, -0.7131, -0.0420,  0.5108,  0.7955, -1.0939,  0.6354,  0.3302,\n",
      "          1.0080, -0.7184, -0.2159, -0.0087]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[ProtoTensor(\n",
      "data: ProtoTensor([[0.1479],\n",
      "             [0.1865]], device='cuda:0', grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3]), tensor([[0.1981],\n",
      "        [0.4187],\n",
      "        [0.0199]], device='cuda:0', grad_fn=<AliasBackward0>)]\n",
      "ProtoTensor(\n",
      "data: ProtoTensor([[-0.4357, -0.3683,  0.6312, -0.6219, -0.2018, -0.5183, -0.2090,\n",
      "              -0.3353, -0.1497,  0.4345],\n",
      "             [-0.2091, -0.4728,  0.3515, -0.3201, -0.3234, -0.0273, -0.6198,\n",
      "               0.3578, -0.0327,  0.2757]], device='cuda:0',\n",
      "            grad_fn=<AliasBackward0>)\n",
      "tag_stack: [tensor([[0],\n",
      "        [2]], device='cuda:0')]\n",
      "load stack: [3])\n",
      "tensor([[ 0.0270,  0.6261, -0.3009, -0.5337, -0.3163,  0.3721,  0.7439,  0.3995,\n",
      "          0.1930, -0.2039, -0.0100, -0.0818,  0.8748, -1.0842, -0.1366, -0.2622,\n",
      "          0.1401,  0.2910,  0.1631,  0.0212],\n",
      "        [-0.7049,  0.1113, -0.1486, -0.8411, -0.6779,  0.6392,  0.4571, -0.6143,\n",
      "          0.5018, -0.3381,  0.1328, -0.6396,  0.4353, -0.5989,  0.0158, -0.2000,\n",
      "          0.3147,  0.7722, -0.1891,  0.9752],\n",
      "        [ 0.3780,  0.4624,  0.3799, -0.0680, -0.1261, -0.2054,  0.2106, -0.3654,\n",
      "          0.2801,  0.3465,  0.1957, -0.0186,  0.1083, -0.3351,  0.2381, -0.4891,\n",
      "         -0.2773,  0.0172, -0.0700, -0.2451]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True,\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x, route_gates_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_0 = route_gates_x[0] * x_0\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        x_1 = route_gates_x[1] * x_1\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model.cuda()\n",
    "for i in range(10):\n",
    "    x = torch.randn((3, 10)).cuda()\n",
    "    y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "    x, y = dy_model(x, y)\n",
    "\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 4-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 4, 2, 2])\n",
      "torch.Size([3, 8, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "route_func = (\n",
    "    lambda x: nn.Sequential(\n",
    "        nn.Conv2d(4, 2, 1),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Conv2d(2, 2, 1),\n",
    "    )\n",
    "    .cuda()(x)\n",
    "    .view(-1, 2)\n",
    ")  # [bs x dst_num x 1 x 1] keep up down -> keep down\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self, dst_num):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(\n",
    "            nn.Conv2d(4, 2, 1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(2, 2, 1),\n",
    "        )\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True,\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\",\n",
    "            fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert2 = nn.Conv2d(4, 8, 1)\n",
    "        self.expert3 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert4 = nn.Conv2d(4, 8, 1)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        gates_x = self.route_func(x).view(-1, 2)\n",
    "        gates_y = self.route_func(y).view(-1, 2)\n",
    "        route_results_x, _ = self.scatter_router_0(x, gates_x)\n",
    "        route_results_y = self.scatter_router_1(y, gates_y)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting(2)\n",
    "dy_model.cuda()\n",
    "for i in range(10):\n",
    "    x = torch.randn((3, 4, 2, 2)).cuda()\n",
    "    y = torch.randn((3, 4, 2, 2)).cuda()\n",
    "    x, y = dy_model(x, y)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4914c35dbc1a262acb2241fbfc193aaeb9362d455da2cebdd4b0a1d658dbfd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
