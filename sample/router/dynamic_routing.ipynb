{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_x: tensor([[ 0.1721,  0.2055, -0.8656,  1.4035,  0.3877,  1.3031, -1.0075, -1.0683,\n",
      "          1.9448,  0.2695],\n",
      "        [-0.6468, -1.2882,  0.1866,  2.0845,  0.1092,  0.5862,  0.9875, -0.5144,\n",
      "          0.7807,  0.2720],\n",
      "        [-0.2088,  1.6116,  0.9699, -1.0278,  0.7947,  1.2612, -1.0729,  0.9025,\n",
      "         -0.9402, -0.6119],\n",
      "        [-0.6897, -0.0112,  0.8530, -0.1993, -0.3786, -0.1821,  0.8073,  0.1916,\n",
      "          0.1905, -0.5488]], device='cuda:0')\n",
      "GridTensor([[0, 1],\n",
      "            [1, 1],\n",
      "            [0, 1],\n",
      "            [1, 1]], device='cuda:0', dtype=torch.int32)\n",
      "tag_stack: [None]\n",
      "load stack: [None]\n",
      "extra_attr_dict: {} None False True False\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [0, 3],\n",
      "        [2, 4]], device='cuda:0', dtype=torch.int32) tensor([2, 4], device='cuda:0', dtype=torch.int32)\n",
      "x_0: GridTensor([[-0.7577,  1.0721,  0.1135, -0.1744, -0.3359, -0.3534, -0.3849,\n",
      "             -0.5662, -0.9574,  0.4290],\n",
      "            [-0.6462, -0.1764,  0.3958,  0.2417, -0.5441,  0.2635, -0.0199,\n",
      "             -0.7194, -0.2627, -0.3004]], device='cuda:0')\n",
      "tag_stack: [tensor([2, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([2], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "x_1: GridTensor([[ 0.2737, -0.0670,  0.6543, -0.9605, -0.1550,  0.2179, -0.3978,\n",
      "             -1.1199, -0.7477, -0.5451],\n",
      "            [-0.2561,  0.2373,  0.3929, -0.3675, -0.6040, -0.3443,  0.0770,\n",
      "             -0.8908, -0.2172, -0.8593],\n",
      "            [ 0.3923, -1.1334, -0.2070,  0.7861,  0.9876,  0.9610, -0.5117,\n",
      "             -1.0311, -0.2549, -0.3190],\n",
      "            [ 0.2067, -0.0967, -0.4201,  0.2858,  0.1305, -0.0995, -0.2767,\n",
      "             -0.2420,  0.1512, -0.4938]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 2, 3, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([4], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "tensor([1, 2, 3, 4], device='cuda:0', dtype=torch.int32) tensor([4], device='cuda:0', dtype=torch.int32)\n",
      "GridTensor([[ 0.2737, -0.0670,  0.6543, -0.9605, -0.1550,  0.2179, -0.3978,\n",
      "             -1.1199, -0.7477, -0.5451],\n",
      "            [-1.0138,  1.3094,  0.5064, -0.5418, -0.9400, -0.6977, -0.3079,\n",
      "             -1.4570, -1.1747, -0.4303],\n",
      "            [ 0.3923, -1.1334, -0.2070,  0.7861,  0.9876,  0.9610, -0.5117,\n",
      "             -1.0311, -0.2549, -0.3190],\n",
      "            [-0.4394, -0.2732, -0.0243,  0.5275, -0.4136,  0.1640, -0.2966,\n",
      "             -0.9614, -0.1115, -0.7942]], device='cuda:0')\n",
      "tag_stack: [tensor([1, 2, 3, 4], device='cuda:0', dtype=torch.int32)]\n",
      "load stack: [tensor([4], device='cuda:0', dtype=torch.int32)]\n",
      "extra_attr_dict: {}\n",
      "<function _assert_is_none at 0x7fa19d9131f0>\n",
      "<function annotate at 0x7fa18fcd8700>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mprint\u001b[39m(results)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbrt\u001b[39;00m \u001b[39mimport\u001b[39;00m symbolic_trace\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m traced_dy_model \u001b[39m=\u001b[39m symbolic_trace(dy_model)\n",
      "File \u001b[0;32m~/brainstorm_project/brainstorm/python/brt/trace/graph.py:27\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(m, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m     24\u001b[0m     m, nn\u001b[39m.\u001b[39mModule\n\u001b[1;32m     25\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mbrt provided symbolic_trace only works on nn.Modules\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m tracer \u001b[39m=\u001b[39m GraphTracer()\n\u001b[0;32m---> 27\u001b[0m graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace(m)\n\u001b[1;32m     28\u001b[0m name \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m name\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m fx\u001b[39m.\u001b[39mGraphModule(tracer\u001b[39m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.8-4.11.0/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:587\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    586\u001b[0m         _autowrap_check(patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids)\n\u001b[0;32m--> 587\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),), {},\n\u001b[1;32m    588\u001b[0m                      type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    590\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\n",
      "\u001b[1;32m/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb Cell 1\u001b[0m in \u001b[0;36mDynamicRouting.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m annotate(x, [\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     x_gates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroute_func(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2252545833303930227d/home/whcui/brainstorm_project/brainstorm/sample/router/dynamic_routing.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     route_results_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter_router(x, x_gates)\n",
      "File \u001b[0;32m~/brainstorm_project/brainstorm/python/brt/runtime/grid_tensor.py:347\u001b[0m, in \u001b[0;36mannotate\u001b[0;34m(t, dims, cell_shape)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(dims, \u001b[39mlist\u001b[39m)\n\u001b[1;32m    346\u001b[0m dims \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(dims)\n\u001b[0;32m--> 347\u001b[0m \u001b[39massert\u001b[39;00m dims[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39;49m(t\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m cell_shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(cell_shape, \u001b[39mlist\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.8-4.11.0/lib/python3.8/site-packages/torch/fx/proxy.py:291\u001b[0m, in \u001b[0;36mProxy.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported in symbolic tracing by default. If you want \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mthis call to be recorded, please call torch.fx.wrap(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m'\u001b[39m\u001b[39m) at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mmodule scope\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "from brt import annotate\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)  # keep\n",
    "        self.expert2 = nn.Linear(10, 10)  # upsample\n",
    "        self.gather_router = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = annotate(x, [0])\n",
    "        x_gates = self.route_func(x)\n",
    "        route_results_x = self.scatter_router(x, x_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        print(f\"x_0: {x_0}\")\n",
    "        print(f\"x_1: {x_1}\")\n",
    "        x = self.gather_router([x_0, x_1])\n",
    "        return x\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(1):\n",
    "    in_x = torch.randn((4, 10)).cuda()\n",
    "    print(f\"in_x: {in_x}\")\n",
    "    dy_model = dy_model.cuda().eval()\n",
    "    with torch.inference_mode():\n",
    "        results = dy_model(in_x.cuda())\n",
    "    print(results)\n",
    "\n",
    "from brt import symbolic_trace\n",
    "\n",
    "traced_dy_model = symbolic_trace(dy_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Default Dispatcher and 2-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "import brt\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)  # keep\n",
    "        self.expert2 = nn.Linear(10, 20)  # upsample\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = brt.annotate(x, [0])\n",
    "        y = brt.annotate(y, [0])\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return route_results_x, route_results_y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "\n",
    "for i in range(1):\n",
    "    in_x = torch.randn((4, 10)).cuda()\n",
    "    in_y = torch.randn((4, 10)).cuda()\n",
    "    print(in_x)\n",
    "    print(in_y)\n",
    "\n",
    "    dy_model = dy_model.cuda().eval()\n",
    "    results = dy_model(in_x.cuda(), in_y.cuda())\n",
    "    print(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model.cuda()\n",
    "\n",
    "for i in range(1):\n",
    "    x = torch.randn((3, 10)).cuda()\n",
    "    y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "    x, y = dy_model(x, y)\n",
    "\n",
    "    print(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 2-D tensor while routing gates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(nn.Linear(10, 2), nn.ReLU())\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True, protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Linear(10, 10)\n",
    "        self.expert2 = nn.Linear(10, 20)\n",
    "        self.expert3 = nn.Linear(10, 10)\n",
    "        self.expert4 = nn.Linear(10, 20)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_gates = self.route_func(x)\n",
    "        y_gates = self.route_func(y)\n",
    "        route_results_x, route_gates_x = self.scatter_router_0(x, x_gates)\n",
    "        route_results_y = self.scatter_router_1(y, y_gates)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_0 = route_gates_x[0] * x_0\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        x_1 = route_gates_x[1] * x_1\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting()\n",
    "dy_model.cuda()\n",
    "for i in range(10):\n",
    "    x = torch.randn((3, 10)).cuda()\n",
    "    y = torch.randn((3, 10)).cuda()\n",
    "\n",
    "    x, y = dy_model(x, y)\n",
    "\n",
    "    # print(x.shape)\n",
    "    # print(y.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Routing with Residual Router and 4-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from brt.router import ScatterRouter, GatherRouter\n",
    "\n",
    "\n",
    "route_func = (\n",
    "    lambda x: nn.Sequential(\n",
    "        nn.Conv2d(4, 2, 1), nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(2, 2, 1),\n",
    "    )\n",
    "    .cuda()(x)\n",
    "    .view(-1, 2)\n",
    ")  # [bs x dst_num x 1 x 1] keep up down -> keep down\n",
    "\n",
    "\n",
    "class DynamicRouting(nn.Module):\n",
    "    def __init__(self, dst_num):\n",
    "        super().__init__()\n",
    "        self.route_func = nn.Sequential(\n",
    "            nn.Conv2d(4, 2, 1), nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(2, 2, 1),\n",
    "        )\n",
    "        self.scatter_router_0 = ScatterRouter(\n",
    "            dispatch_score=True, protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.scatter_router_1 = ScatterRouter(\n",
    "            protocol_type=\"threshold\", fabric_type=\"dispatch\",\n",
    "        )\n",
    "        self.expert1 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert2 = nn.Conv2d(4, 8, 1)\n",
    "        self.expert3 = nn.Conv2d(4, 4, 1)\n",
    "        self.expert4 = nn.Conv2d(4, 8, 1)\n",
    "        self.gather_router_0 = GatherRouter(fabric_type=\"combine\")\n",
    "        self.gather_router_1 = GatherRouter(fabric_type=\"combine\")\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        gates_x = self.route_func(x).view(-1, 2)\n",
    "        gates_y = self.route_func(y).view(-1, 2)\n",
    "        route_results_x, _ = self.scatter_router_0(x, gates_x)\n",
    "        route_results_y = self.scatter_router_1(y, gates_y)\n",
    "        x_0 = self.expert1(route_results_x[0])\n",
    "        x_1 = self.expert2(route_results_x[1])\n",
    "        y_0 = self.expert3(route_results_y[0])\n",
    "        y_1 = self.expert4(route_results_y[1])\n",
    "        x = self.gather_router_0([x_0, y_0])\n",
    "        y = self.gather_router_1([x_1, y_1])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dy_model = DynamicRouting(2)\n",
    "dy_model.cuda()\n",
    "for i in range(10):\n",
    "    x = torch.randn((3, 4, 2, 2)).cuda()\n",
    "    y = torch.randn((3, 4, 2, 2)).cuda()\n",
    "    x, y = dy_model(x, y)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4914c35dbc1a262acb2241fbfc193aaeb9362d455da2cebdd4b0a1d658dbfd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
