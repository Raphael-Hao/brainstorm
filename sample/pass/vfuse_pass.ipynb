{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-16 17:05:36] INFO (numexpr.utils/MainThread) Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "[2023-01-16 17:05:36] INFO (numexpr.utils/MainThread) Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2023-01-16 17:05:36] INFO (numexpr.utils/MainThread) NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import fx\n",
    "from torch.fx import GraphModule, Graph, Node\n",
    "\n",
    "import brt\n",
    "from brt.trace import symbolic_trace, GraphTracer\n",
    "\n",
    "# from brt.trace.graph import symbolic_trace\n",
    "from brt.passes import VerticalFusePass\n",
    "from brt.router import ScatterRouter, GatherRouter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.router = ScatterRouter(\n",
    "            fabric_type=\"_fused_dispatch\",\n",
    "            fabric_kwargs={\n",
    "                \"fixed_capacity\": torch.tensor((4, 6), dtype=torch.int32).cuda()\n",
    "            },\n",
    "            capturing=True,\n",
    "            capture_mode=\"max\",\n",
    "        )\n",
    "        # self.y1_bias = torch.nn.Parameter(torch.randn(3, 32, 32))\n",
    "        self.y1_bias = torch.randn(3, 1, 1).cuda()\n",
    "        self.expert0 = nn.Sequential(nn.Conv2d(3, 3, 1), nn.ReLU())\n",
    "        self.expert1 = nn.Sequential(nn.Conv2d(3, 3, 1), nn.ReLU(), nn.Conv2d(3, 3, 1),)\n",
    "        self.gather = GatherRouter()\n",
    "\n",
    "    def forward(self, x=None):\n",
    "        y0, y1 = self.router(\n",
    "            x,\n",
    "            torch.tensor(((3, 0), (2, 3), (0, 1)), dtype=torch.int32).cuda(),\n",
    "            # torch.randint(0, 3, (3, 2)).cuda(),\n",
    "        )\n",
    "        y0 += self.expert0(y0)\n",
    "        y1 = self.expert1(y1)\n",
    "        self.y1_bias += self.y1_bias\n",
    "        y1 += self.y1_bias\n",
    "        y = self.gather([y0, y1])\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x](default=None) | unfixed\n",
      "    %_tensor_constant0 : [#users=1] = get_attr[target=_tensor_constant0] | unfixed\n",
      "    %router : [#users=2] = call_module[target=router](args = (%x, %_tensor_constant0), kwargs = {}) | unfixed\n",
      "    %getitem : [#users=2] = call_function[target=operator.getitem](args = (%router, 0), kwargs = {}) | unfixed\n",
      "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%router, 1), kwargs = {}) | unfixed\n",
      "    %expert0_0 : [#users=1] = call_module[target=expert0.0](args = (%getitem,), kwargs = {}) | unfixed\n",
      "    %expert0_1 : [#users=1] = call_module[target=expert0.1](args = (%expert0_0,), kwargs = {}) | unfixed\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%getitem, %expert0_1), kwargs = {}) | unfixed\n",
      "    %expert1_0 : [#users=1] = call_module[target=expert1.0](args = (%getitem_1,), kwargs = {}) | unfixed\n",
      "    %expert1_1 : [#users=1] = call_module[target=expert1.1](args = (%expert1_0,), kwargs = {}) | unfixed\n",
      "    %expert1_2 : [#users=1] = call_module[target=expert1.2](args = (%expert1_1,), kwargs = {}) | unfixed\n",
      "    %y1_bias : [#users=1] = get_attr[target=y1_bias] | unfixed\n",
      "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%expert1_2, %y1_bias), kwargs = {}) | unfixed\n",
      "    %gather : [#users=0] = call_module[target=gather](args = ([%add, %add_1],), kwargs = {}) | unfixed\n",
      "[DEBUG] router node `router` found\n",
      "[DEBUG] fuse node `expert0_0`\n",
      "[DEBUG] fuse node `expert0_1`\n",
      "[DEBUG] fuse node `add`\n",
      "[DEBUG] router node `gather` found\n",
      "[DEBUG] node `expert0_1` already fused\n",
      "[DEBUG] node `add` already fused\n",
      "[DEBUG] fuse node `expert1_0`\n",
      "[DEBUG] fuse node `expert1_1`\n",
      "[DEBUG] find jit module failed\n",
      "[DEBUG] node `expert1_1` already fused\n",
      "[DEBUG] fuse node `expert1_2`\n",
      "[DEBUG] node `add_1` is not fixed\n",
      "[DEBUG] start fusing\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No kernel found in database with identifier = '{\"device_name\": \"NVIDIA_GeForce_RTX_3090\", \"input_infos\": {\"input_0\": [4, 3, 1, 1]}, \"method\": \"forward\", \"op_type\": \"Conv2dBiasReLUAdd\", \"output_infos\": {\"output_0\": [4, 3, 1, 1]}, \"parameters\": {\"dilation\": [1, 1], \"groups\": 1, \"in_channels\": 3, \"kernel_size\": [1, 1], \"out_channels\": 3, \"padding\": [0, 0], \"stride\": [1, 1]}}', objective_func = 'fastest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ouyang/project/brainstorm/vfuse_pass.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(gm\u001b[39m.\u001b[39mgraph)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m vertical_fuse_pass \u001b[39m=\u001b[39m VerticalFusePass(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     m, sample_inputs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mrandn(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()}, fixing_scatters\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fixed_inputs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m vertical_fuse_pass\u001b[39m.\u001b[39;49mrun_on_graph()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m gm \u001b[39m=\u001b[39m vertical_fuse_pass\u001b[39m.\u001b[39mfinalize()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinke_edge_2/home/ouyang/project/brainstorm/vfuse_pass.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(gm\u001b[39m.\u001b[39mgraph)\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/passes/vertical_fuse.py:303\u001b[0m, in \u001b[0;36mVerticalFusePass.run_on_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_mod\u001b[39m.\u001b[39mrecompile()\n\u001b[1;32m    302\u001b[0m \u001b[39m# TODO: make module\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m fused_jit_module \u001b[39m=\u001b[39m make_jit_module(\n\u001b[1;32m    304\u001b[0m     modules\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph_mod,\n\u001b[1;32m    305\u001b[0m     \u001b[39m# TODO\u001b[39;49;00m\n\u001b[1;32m    306\u001b[0m     sample_inputs\u001b[39m=\u001b[39;49mfused_node_args_sample_inputs,\n\u001b[1;32m    307\u001b[0m )\n\u001b[1;32m    308\u001b[0m fused_module_target \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfused:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m&\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    309\u001b[0m     fpn\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m fpn \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39mfuse_parteners\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_mod\u001b[39m.\u001b[39madd_submodule(fused_module_target, fused_jit_module)\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/factory.py:71\u001b[0m, in \u001b[0;36mmake_jit_module\u001b[0;34m(modules, sample_inputs, opt_level, objective_func, rank)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_jit_module\u001b[39m(\n\u001b[1;32m     65\u001b[0m     modules: Union[nn\u001b[39m.\u001b[39mModule, nn\u001b[39m.\u001b[39mModuleList],\n\u001b[1;32m     66\u001b[0m     sample_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     rank: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     70\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m nn\u001b[39m.\u001b[39mModule:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m ModuleFactory\u001b[39m.\u001b[39;49mmake_module(\n\u001b[1;32m     72\u001b[0m         modules,\n\u001b[1;32m     73\u001b[0m         sample_inputs\u001b[39m=\u001b[39;49msample_inputs,\n\u001b[1;32m     74\u001b[0m         opt_level\u001b[39m=\u001b[39;49mopt_level,\n\u001b[1;32m     75\u001b[0m         objective_func\u001b[39m=\u001b[39;49mobjective_func,\n\u001b[1;32m     76\u001b[0m         rank\u001b[39m=\u001b[39;49mrank,\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/factory.py:124\u001b[0m, in \u001b[0;36mModuleFactory.make_module\u001b[0;34m(modules, sample_inputs, opt_level, objective_func, rank)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_module\u001b[39m(\n\u001b[1;32m    117\u001b[0m     modules,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     rank: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    122\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m nn\u001b[39m.\u001b[39mModule:\n\u001b[1;32m    123\u001b[0m     jit_module \u001b[39m=\u001b[39m JitModuleFactory\u001b[39m.\u001b[39mproduce(modules, opt_level)\n\u001b[0;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m jit_module\u001b[39m.\u001b[39;49mmake_module(\n\u001b[1;32m    125\u001b[0m         sample_inputs\u001b[39m=\u001b[39;49msample_inputs,\n\u001b[1;32m    126\u001b[0m         objective_func\u001b[39m=\u001b[39;49mobjective_func,\n\u001b[1;32m    127\u001b[0m         rank\u001b[39m=\u001b[39;49mrank,\n\u001b[1;32m    128\u001b[0m     )\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/modules/conv2d_element_wise.py:209\u001b[0m, in \u001b[0;36mConv2dElementWiseModule.make_module\u001b[0;34m(self, sample_inputs, objective_func, rank)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_module\u001b[39m(\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    205\u001b[0m     sample_inputs: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    206\u001b[0m     objective_func: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfastest\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    207\u001b[0m     rank: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    208\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m nn\u001b[39m.\u001b[39mModule:\n\u001b[0;32m--> 209\u001b[0m     jit_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_function(\n\u001b[1;32m    210\u001b[0m         sample_inputs\u001b[39m=\u001b[39;49msample_inputs,\n\u001b[1;32m    211\u001b[0m         mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meval\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    212\u001b[0m         objective_func\u001b[39m=\u001b[39;49mobjective_func,\n\u001b[1;32m    213\u001b[0m         rank\u001b[39m=\u001b[39;49mrank,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m     module_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBRT.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_name\n\u001b[1;32m    216\u001b[0m     extra_repr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv2d\u001b[39m.\u001b[39mextra_repr()\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/modules/atom.py:37\u001b[0m, in \u001b[0;36mAtomModule.make_function\u001b[0;34m(self, sample_inputs, mode, objective_func, rank)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_function\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     sample_inputs: AtomModuleInputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     rank: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m autograd\u001b[39m.\u001b[39mFunction:\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m         jit_kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_kernel(\n\u001b[1;32m     38\u001b[0m             sample_inputs, \u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m, objective_func, rank\n\u001b[1;32m     39\u001b[0m         )\n\u001b[1;32m     40\u001b[0m         (\n\u001b[1;32m     41\u001b[0m             input_arg_num,\n\u001b[1;32m     42\u001b[0m             total_arg_num,\n\u001b[1;32m     43\u001b[0m             input_arg_indices,\n\u001b[1;32m     44\u001b[0m             output_arg_indices,\n\u001b[1;32m     45\u001b[0m         ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_arg_infos(\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         out_data \u001b[39m=\u001b[39m [\n\u001b[1;32m     47\u001b[0m             torch\u001b[39m.\u001b[39mempty(shp, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m             \u001b[39mfor\u001b[39;00m shp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_shape(\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, sample_inputs)\n\u001b[1;32m     49\u001b[0m         ]\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/modules/base.py:33\u001b[0m, in \u001b[0;36mModuleBase.make_kernel\u001b[0;34m(self, sample_inputs, method, objective_func, rank)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_kernel\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     28\u001b[0m     sample_inputs: ModuleInputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     rank: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m]:\n\u001b[0;32m---> 33\u001b[0m     global_kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_global_kernel(\n\u001b[1;32m     34\u001b[0m         sample_inputs\u001b[39m=\u001b[39;49msample_inputs,\n\u001b[1;32m     35\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m     36\u001b[0m         objective_func\u001b[39m=\u001b[39;49mobjective_func,\n\u001b[1;32m     37\u001b[0m         rank\u001b[39m=\u001b[39;49mrank,\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     kernel_code, _, _, _ \u001b[39m=\u001b[39m global_kernel\u001b[39m.\u001b[39mget_code()\n\u001b[1;32m     40\u001b[0m     processed_template_fname \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m     41\u001b[0m         BRT_KERNEL_TEMPLATE_PATH\n\u001b[1;32m     42\u001b[0m         \u001b[39m/\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mprocessed_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m global_kernel\u001b[39m.\u001b[39mfunc_name[:\u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.cu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     )\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/modules/conv2d_element_wise.py:194\u001b[0m, in \u001b[0;36mConv2dElementWiseModule._make_global_kernel\u001b[0;34m(self, sample_inputs, method, objective_func, rank)\u001b[0m\n\u001b[1;32m    185\u001b[0m         output_infos \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39moutput_0\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mlist\u001b[39m(sample_output\u001b[39m.\u001b[39mshape)}\n\u001b[1;32m    186\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    187\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[39mmodule name: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_name\u001b[39m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         )\n\u001b[0;32m--> 194\u001b[0m         \u001b[39mreturn\u001b[39;00m ModuleKernel(\n\u001b[1;32m    195\u001b[0m             module_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_name,\n\u001b[1;32m    196\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    197\u001b[0m             kernel_source\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    198\u001b[0m             input_infos\u001b[39m=\u001b[39;49minput_infos,\n\u001b[1;32m    199\u001b[0m             output_infos\u001b[39m=\u001b[39;49moutput_infos,\n\u001b[1;32m    200\u001b[0m             parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    201\u001b[0m         )\u001b[39m.\u001b[39;49mload_from_db(objective_func, rank)\n",
      "File \u001b[0;32m~/project/brainstorm/python/brt/jit/codegen/module.py:246\u001b[0m, in \u001b[0;36mModuleKernel.load_from_db\u001b[0;34m(self, objective_func, rank)\u001b[0m\n\u001b[1;32m    242\u001b[0m fetched_kernel \u001b[39m=\u001b[39m kernel_storager\u001b[39m.\u001b[39mquery_kernel(\n\u001b[1;32m    243\u001b[0m     identifier, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplatform, objective_func, rank\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m fetched_kernel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo kernel found in database with \u001b[39m\u001b[39m{\u001b[39;00midentifier \u001b[39m= }\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mobjective_func \u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m attribute_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(fetched_kernel[\u001b[39m3\u001b[39m])\n\u001b[1;32m    250\u001b[0m function_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(fetched_kernel[\u001b[39m6\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: No kernel found in database with identifier = '{\"device_name\": \"NVIDIA_GeForce_RTX_3090\", \"input_infos\": {\"input_0\": [4, 3, 1, 1]}, \"method\": \"forward\", \"op_type\": \"Conv2dBiasReLUAdd\", \"output_infos\": {\"output_0\": [4, 3, 1, 1]}, \"parameters\": {\"dilation\": [1, 1], \"groups\": 1, \"in_channels\": 3, \"kernel_size\": [1, 1], \"out_channels\": 3, \"padding\": [0, 0], \"stride\": [1, 1]}}', objective_func = 'fastest'"
     ]
    }
   ],
   "source": [
    "m = TestModule().eval().cuda()\n",
    "gm = symbolic_trace(m)\n",
    "print(gm.graph)\n",
    "vertical_fuse_pass = VerticalFusePass(\n",
    "    m, sample_inputs={\"x\": torch.randn(3, 3, 1, 1).cuda()}, fixing_scatters=True, fixed_inputs=True\n",
    ")\n",
    "vertical_fuse_pass.run_on_graph()\n",
    "gm = vertical_fuse_pass.finalize()\n",
    "print(gm.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = TestModule().eval().cuda()\n",
    "for _ in range(2):\n",
    "    m(torch.randn(3, 3, 1, 1).cuda())\n",
    "print(m.router.load_history)\n",
    "gm = symbolic_trace(\n",
    "    m,\n",
    "    tracing_shape=True,\n",
    "    sample_inputs={\"x\": torch.randn(3, 3, 1, 1).cuda()},\n",
    "    fixed_inputs=True,\n",
    ")\n",
    "print(gm.graph)\n",
    "for node in gm.graph.nodes:\n",
    "    print(node.name, node.is_fixed_inout, node.inshape, node.outshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.livesr.archs.nas_mdsr import SingleNetwork as nas_mdsr\n",
    "\n",
    "livesr = nas_mdsr(1, 3, 3, 4, 2).eval()\n",
    "print(livesr)\n",
    "gm_livesr = symbolic_trace(livesr)\n",
    "print(gm_livesr.graph)\n",
    "vertical_fuse_pass = VerticalFusePass(gm_livesr)\n",
    "vertical_fuse_pass.run_on_graph()\n",
    "gm_livesr = vertical_fuse_pass.finalize()\n",
    "print(gm_livesr.graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f996d3760d2c10fdcf00d63f53b2493ce402f07df13c0e840400296087e73974"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
